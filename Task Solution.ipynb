{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39de6d95-5e8c-4308-9ce6-63a98d65c879",
   "metadata": {},
   "source": [
    "# Evaluation of Segmentation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6afaca-54dc-4d1a-9af8-d6d311f059b9",
   "metadata": {},
   "source": [
    "- Thank you team for providing me the task I was able to experience the use of fiftyone which is kind of pretty kool for computer vision tasks\n",
    "- I used fiftyone libraries components to complete the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baafcd4-e9cf-4fdb-b5cf-ecc35b0823e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install fiftyone shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349037f2-612e-437d-9387-9d151afa6d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from shapely.geometry import box\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfeb6a9c-d053-4bb8-9240-d6c9c7e53c03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/yuvi_dh/fiftyone/open-images-v7/validation' if necessary\n",
      "Only found 35 (<100) samples matching your requirements\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [3.1s elapsed, 0s remaining, 11.6 samples/s]      \n",
      "Dataset 'open-images-apples-validset' created\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"open-images-apples-validset\"\n",
    "\n",
    "try:\n",
    "    # Load previously downloaded dataset.\n",
    "    dataset = fo.load_dataset(dataset_name).delete()\n",
    "except Exception:\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        \"open-images-v7\",\n",
    "        split=\"validation\",\n",
    "        label_types=[\"segmentations\"],\n",
    "        classes = [\"Apple\"],\n",
    "        max_samples=100,\n",
    "        seed=51,\n",
    "        shuffle=True,\n",
    "        dataset_name=dataset_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e22652-7f6a-484c-a675-4704d2efe0ed",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "- Write code that finds instances where the masks overlap and visualize some positive and negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881d1991-3b8e-4200-a2a7-47fed3d8f9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overlapping_samples = list()\n",
    "\n",
    "# We will iterate over the dataset samples one by one.\n",
    "for sample in dataset:\n",
    "    # Obtaining the detections for each of the samples\n",
    "    detections = sample.ground_truth\n",
    "    \n",
    "    # Obtaining the list of bounding boxes for each of the object present in an image\n",
    "    bboxes = [detection.bounding_box for detection in detections.detections]\n",
    "    \n",
    "    # Iteration over each pair of boxes in an image and checking whether there is an overlap of any object with any other object in the whole set of objects\n",
    "    for i in range(len(bboxes)):\n",
    "        bbox1 = bboxes[i]\n",
    "        for j in range(i+1, len(bboxes)):\n",
    "            bbox2 = bboxes[j]\n",
    "            \n",
    "            # Convert bounding boxes to Shapely box object.\n",
    "            box1 = box(bbox1[0], bbox1[1], bbox1[2], bbox1[3])\n",
    "            box2 = box(bbox2[0], bbox2[1], bbox2[2], bbox2[3])\n",
    "            \n",
    "            # Check if the boxes overlap by Shapely box intersect method\n",
    "            if box1.intersects(box2):\n",
    "                overlapping_samples.append(sample.id)\n",
    "                \n",
    "\n",
    "\n",
    "# Id's of all the images\n",
    "all_ids = set([sample.id for sample in dataset])\n",
    "\n",
    "# Selecting Image Id's that will only contain atleast 2 or more overlapping objects\n",
    "overlapping_samples = set(overlapping_samples)\n",
    "\n",
    "# Selecting Image Id's that will only not overlapping objects\n",
    "non_overlapping_samples = all_ids - overlapping_samples\n",
    "\n",
    "# Creating a view for overlapping samples\n",
    "overlaps = dataset.select(overlapping_samples)\n",
    "dataset.save_view(\"Overlap View\",overlaps)\n",
    "\n",
    "# Creating a view for overlapping samples\n",
    "non_overlaps = dataset.select(non_overlapping_samples)\n",
    "dataset.save_view(\"Non-Overlap View\",non_overlaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15bbce-ca07-4cae-b730-029b2313290c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 2\n",
    "- Remove images that also show objects that are not labeled as apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569c5b40-54fb-4529-93b8-ef939d7f5ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All image Id's\n",
    "all_ids = set([sample.id for sample in dataset])\n",
    "# Selecting Image Id's that contains objects other than apples\n",
    "nap = dataset.select_fields(\"ground_truth\").filter_labels('ground_truth', F(\"label\") != 'Apple') \n",
    "\n",
    "# Get the IDs of images with apples\n",
    "nap_ids = set([sample.id for sample in nap])\n",
    "ap_ids = all_ids - nap_ids\n",
    "\n",
    "# Creating a view for ap_ids\n",
    "apples_view = dataset.select(ap_ids)\n",
    "dataset.save_view(\"Only Apples View\",apples_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd902aa-95d2-40e3-948c-04500b60948d",
   "metadata": {},
   "source": [
    "### Viewing the saved views\n",
    "- Overlap View: Images that consist of objects with masks that have overlap\n",
    "- Non-Overlap View: Images that consist of objects with masks that don't have any overlap\n",
    "- Only Applies View: Images that only consists of apple labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e56d84-4236-4b6f-a8b8-7eaac283f01e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=2016e325-95e6-4048-91eb-10cd9c3e374a\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f432564d0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d6075-e75a-4e48-bed8-4a1167ddb6bd",
   "metadata": {},
   "source": [
    "- Overlap and Non-Overlap views are not perfect, few are wrongly classified, But that was the best solution I was able to come up with ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fe795-6212-4c13-bfcc-e63e3b97fa59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 3 \n",
    "- Which model would you use to train an instance segmentation model for this dataset and which steps are necessary to train it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c2bf1-2467-4e43-bb6a-bc623faf267b",
   "metadata": {},
   "source": [
    "I am considering that we need to do instance segmentation specifically for the apples\n",
    "- Data Preparation: I will prepare the dataset by annotating the images with instance-level masks for each apple in the images. This will involve manually segmenting the apples and creating mask annotations for apples\n",
    "- Performing Data Augmentation: We just have around 35 images, so probably using augmentation techniques like rotating, vertical or horizontal flips might be good on mask annotations and the base images\n",
    "- Dataset Split: Later I will divide dataset into training and validation sets. Typically 75%:25% split ration\n",
    "- Model Selection: While reading I got to know that Mask-R-CNN is a state of the art model for such tasks, I would use it and try 2 architecture variants of it, i.e. one un-trained, and one pretrained\n",
    "- Model configuration: For the pretrained model, I will fine tune only the last layers, to use the pre-trained weights while training and for model trained from scratch, I will simply initiate the training. In both of them, we would like to have 1 class of apples to be segmented\n",
    "- Data Loading: Loading both the image data and corresponding mask annotations for each apple in images\n",
    "- Model Training: Training the Mask R-CNN model using the prepared dataset. During this training the model learns to detect and segment objects based on the provided mask annotations. The model will optimize the loss function, typically a combination of classification loss and segmentation loss\n",
    "- Validation: During the training we can even pass the validation split of the data and see model's performance on this validation data\n",
    "- Evaluation: We can calculate metrics like mean average precision to get to know the model's accuracy and ability to segment apples' correctly\n",
    "- Model Fine-Tuning: If the model's performance is not satisfactory, we can fine-tune the model, by adjusting hyper-params\n",
    "- Inference: Once the model is trained and validated, we can use it for inference on new unseen images. The trained model will detect and segment objects in the images, including apples it has been trained on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
