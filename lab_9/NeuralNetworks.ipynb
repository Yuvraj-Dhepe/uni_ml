{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3YDtSxjptXt"
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this exercise you will learn how to implement a feedforward neural network and train it with backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TOF5ehVhptXv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "from numpy.random import uniform\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo6F_ZTVptXw"
   },
   "source": [
    "We define two helper functions \"init_toy_data\" and \"init_model\" to create a simple data set to work on and a 2 layer neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rje5MYtptXx"
   },
   "source": [
    "First, we create toy data with categorical labels by sampling from different multivariate normal distributions for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "82g8WBTXptXx"
   },
   "outputs": [],
   "source": [
    "def init_toy_data(num_samples,num_features, num_classes, seed=3):\n",
    "    # num_samples: number of samples *per class*\n",
    "    # num_features: number of features (excluding bias)\n",
    "    # num_classes: number of class labels\n",
    "    # seed: random seed\n",
    "    np.random.seed(seed)\n",
    "    X=np.zeros((num_samples*num_classes, num_features))\n",
    "    y=np.zeros(num_samples*num_classes)\n",
    "    for c in range(num_classes):\n",
    "        # initialize multivariate normal distribution for this class:\n",
    "        # choose a mean for each feature\n",
    "        means = uniform(low=-10, high=10, size=num_features)\n",
    "        # choose a variance for each feature\n",
    "        var = uniform(low=1.0, high=5, size=num_features)\n",
    "        # for simplicity, all features are uncorrelated (covariance between any two features is 0)\n",
    "        cov = var * np.eye(num_features)\n",
    "        # draw samples from normal distribution\n",
    "        X[c*num_samples:c*num_samples+num_samples,:] = multivariate_normal(means, cov, size=num_samples)\n",
    "        # set label\n",
    "        y[c*num_samples:c*num_samples+num_samples] = c\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y28dcb7bptXy"
   },
   "outputs": [],
   "source": [
    "def init_model(input_size,hidden_size,num_classes, seed=3):\n",
    "    # input size: number of input features\n",
    "    # hidden_size: number of units in the hidden layer\n",
    "    # num_classes: number of class labels, i.e., number of output units\n",
    "    np.random.seed(seed)\n",
    "    model = {}\n",
    "    # initialize weight matrices and biases randomly\n",
    "    model['W1'] = uniform(low=-1, high=1, size=(input_size, hidden_size))\n",
    "    model['b1'] = uniform(low=-1, high=1, size=hidden_size)\n",
    "    model['W2'] = uniform(low=-1, high=1, size=(hidden_size, num_classes))\n",
    "    model['b2'] = uniform(low=-1, high=1, size=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2eip8kX7ptXz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[ 0.39636145  1.09468144 -0.89360845  0.91815536]\n",
      " [ 0.94419323 -0.94027869  1.22268078  1.29597409]\n",
      " [-1.41577399  1.15477931 -0.62099631  0.08323307]\n",
      " [-1.35264614 -0.13598976 -1.14221784  0.26928935]\n",
      " [ 0.9352123   0.38225626  1.419864   -1.51152157]\n",
      " [ 0.49265316 -1.55544856  0.01427781 -1.0551303 ]]\n",
      "y: [0. 0. 1. 1. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# create toy data\n",
    "X,y= init_toy_data(2,4,3) # 2 samples per class; 4 features, 3 classes\n",
    "# Normalize data\n",
    "X = zscore(X, axis=0)\n",
    "print('X: ' + str(X))\n",
    "print('y: ' + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpCCDC2mptXz"
   },
   "source": [
    "We now initialise our neural net with one hidden layer consisting of $10$ units and and an output layer consisting of $3$ units. Here we expect (any number of) training samples with $4$ features. We do not apply any activation functions yet. The following figure shows a graphical representation of this neuronal net. \n",
    "<img src=\"nn.graphviz.png\"  width=\"30%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "99BAxa6RptX0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: {'W1': array([[ 0.10159581,  0.41629565, -0.41819052,  0.02165521,  0.78589391,\n",
      "         0.79258618, -0.74882938, -0.58551424, -0.89706559, -0.11838031],\n",
      "       [-0.94024758, -0.08633355,  0.2982881 , -0.44302543,  0.3525098 ,\n",
      "         0.18172563, -0.95203624,  0.11770818, -0.48149511, -0.16979761],\n",
      "       [-0.43294984,  0.38627584, -0.11909256, -0.68626452,  0.08929804,\n",
      "         0.56062953, -0.38727294, -0.55608423, -0.22405748,  0.8727673 ],\n",
      "       [ 0.95199084,  0.34476735,  0.80566822,  0.69150174, -0.24401192,\n",
      "        -0.81556598,  0.30682181,  0.11568152, -0.27687047, -0.54989099]]), 'b1': array([-0.18696017, -0.0621195 , -0.46152884, -0.41641445, -0.0846272 ,\n",
      "        0.72106783,  0.17250581, -0.43302428, -0.44404499, -0.09075585]), 'W2': array([[-0.58917931, -0.59724258,  0.02807012],\n",
      "       [-0.82554126, -0.03282894, -0.27564758],\n",
      "       [ 0.41537324,  0.49349245,  0.38218584],\n",
      "       [ 0.37836083, -0.25279975,  0.33626961],\n",
      "       [-0.32030267,  0.14558774, -0.34838568],\n",
      "       [-0.1097099 , -0.87694214, -0.51464916],\n",
      "       [ 0.94320521, -0.53883159,  0.38295502],\n",
      "       [ 0.30095372,  0.44787828, -0.04982278],\n",
      "       [ 0.19332755, -0.86606115, -0.85487572],\n",
      "       [-0.60204795, -0.69627801, -0.79979131]]), 'b2': array([-0.74141227,  0.10655546, -0.62437035])}\n",
      "model['W1'].shape: (4, 10)\n",
      "model['W2'].shape: (10, 3)\n",
      "model['b1'].shape: (10,)\n",
      "model['b12'].shape: (3,)\n",
      "number of parameters: 83\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = init_model(input_size=4, hidden_size=10, num_classes=3)\n",
    "\n",
    "print('model: ' + str(model))\n",
    "print('model[\\'W1\\'].shape: ' + str(model['W1'].shape))\n",
    "print('model[\\'W2\\'].shape: ' + str(model['W2'].shape))\n",
    "print('model[\\'b1\\'].shape: ' + str(model['b1'].shape))\n",
    "print('model[\\'b12\\'].shape: ' + str(model['b2'].shape))\n",
    "print('number of parameters: ' + str((model['W1'].shape[0] * model['W1'].shape[1]) + \n",
    "     np.sum(model['W2'].shape[0] * model['W2'].shape[1]) + \n",
    "     np.sum(model['b1'].shape[0]) +\n",
    "     np.sum(model['b2'].shape[0] )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojs6ScguptX1"
   },
   "source": [
    "<b>Exercise 1</b>: Implement softmax layer.\n",
    "\n",
    "Implement the softmax function given by \n",
    "\n",
    "### $softmax(x_i) = \\frac{e^{x_i}}{{\\sum_{j\\in 1...J}e^{x_j}}}$, \n",
    "\n",
    "where $J$ is the total number of classes, i.e. the length of  **x** .\n",
    "\n",
    "Note: Implement the function such that it takes a matrix X of shape (N, J) as input rather than a single instance **x**; N is the number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GcgC5wMvptX1"
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp_vals = np.exp(X)\n",
    "    sum_exp_vals = np.sum(exp_vals, axis=1, keepdims=True)\n",
    "    softmax_vals = exp_vals / sum_exp_vals\n",
    "    return softmax_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-BcVCIqptX2"
   },
   "source": [
    "Check if everything is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "recCBdmqptX2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing successful.\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.1, 0.7],[0.7,0.4]])\n",
    "exact_softmax = np.array([[ 0.35434369,  0.64565631],\n",
    "                         [ 0.57444252,  0.42555748]])\n",
    "sm = softmax(x)\n",
    "difference = np.sum(np.abs(exact_softmax - sm))\n",
    "try:\n",
    "    assert difference < 0.000001   \n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO0gCmA3ptX3"
   },
   "source": [
    "<b>Exercise 2</b>: Implement the forward propagation algorithm for the model defined above.\n",
    "\n",
    "The activation function of the hidden neurons is a Rectified Linear Unit $relu(x)=max(0,x)$ (to be applied element-wise to the hidden units)\n",
    "The activation function of the output layer is a softmax function as (as implemented in Exercise 1).\n",
    "\n",
    "The function should return both the activation of the hidden units (after having applied the $relu$ activation function) (shape: $(N, num\\_hidden)$) and the softmax model output (shape: $(N, num\\_classes)$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pze-k4-XptX3"
   },
   "outputs": [],
   "source": [
    "def forward_prop(X,model):\n",
    "    W1=model['W1']\n",
    "    b1=model['b1']\n",
    "    W2=model['W2']\n",
    "    b2=model['b2']\n",
    "    ###############################################\n",
    "    hidden_activations = np.maximum(0,np.dot(X,W1)+b1)\n",
    "    probs = softmax(np.dot(hidden_activations, W2) + b2)\n",
    "    ###############################################\n",
    "    return hidden_activations, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SHrLsiylptX3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing successful.\n"
     ]
    }
   ],
   "source": [
    "acts,probs = forward_prop(X, model)\n",
    "correct_probs = np.array([[0.22836388, 0.51816433, 0.25347179],\n",
    "                            [0.15853289, 0.33057078, 0.51089632],\n",
    "                            [0.40710319, 0.41765056, 0.17524624],\n",
    "                            [0.85151353, 0.03656425, 0.11192222],\n",
    "                            [0.66016592, 0.19839791, 0.14143618],\n",
    "                            [0.70362036, 0.08667923, 0.20970041]])\n",
    "\n",
    "# the difference should be very small.\n",
    "difference =  np.sum(np.abs(probs - correct_probs))\n",
    "\n",
    "try:\n",
    "    assert probs.shape==(X.shape[0],len(set(y)))\n",
    "    assert difference < 0.00001   \n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGef2WaLptX4"
   },
   "source": [
    "<b>Exercise 3:</b> \n",
    "\n",
    "How would you train the above defined neural network? Which loss-function would you use? You do not need to implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSP8x8NEptX4"
   },
   "source": [
    "<b>Part 2 (Neural Net using Keras)</b>\n",
    "\n",
    "Instead of implementing the model learning ourselves, we can use the neural network library Keras for Python (https://keras.io/). Keras is an abstraction layer that either builds on top of Theano or Google's Tensorflow. So please install Keras and Tensorflow/Theano for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKozKPKmptX4"
   },
   "source": [
    "<b>Exercise 4:</b>\n",
    "    Implement the same model as above using Keras:\n",
    "    \n",
    "    ** 1 hidden layer à 10 units\n",
    "    ** softmax output layer à three units\n",
    "    ** 4 input features\n",
    "    \n",
    "Compile the model using categorical cross-entropy (also referred to as 'softmax-loss') as loss function and using categorical crossentropy together with categorical accuracy as metrics for runtime evaluation during training.\n",
    "\n",
    "Hint 1: Use the Sequential Class API of Keras (https://keras.io/api/models/sequential/ or https://www.tensorflow.org/guide/keras/sequential_model)\n",
    "\n",
    "Hint 2: You can use the Adam optimizer of Keras for the model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "\n",
    "# Rest of your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pZqK7kxhptX5"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "################################################\n",
    "inputs = Input(shape=(X.shape[1],))\n",
    "hidden = Dense(10, activation='relu', name='Hidden_Layer')(inputs)\n",
    "output = Dense(3, activation='softmax', name='Final_Layer')(hidden)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "################################################\n",
    "\n",
    "# Compile the model\n",
    "################################################\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " Hidden_Layer (Dense)        (None, 10)                50        \n",
      "                                                                 \n",
      " Final_Layer (Dense)         (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83\n",
      "Trainable params: 83\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QhNMmKGptX5"
   },
   "source": [
    "The description of the current network can always be looked at via the summary method. The layers can be accessed via model.layers and weights can be obtained with the method get_weights. Check if your model is as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "abzHV5AxptX5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer weights: [[-0.17057064 -0.52151155 -0.16129223  0.32716012 -0.15255868 -0.00511712\n",
      "   0.30313748 -0.43001586  0.5141635   0.30557483]\n",
      " [ 0.21457195  0.05395317 -0.35099348  0.2160024   0.4591334   0.46954262\n",
      "   0.50378275  0.11918736 -0.18618429  0.59894216]\n",
      " [-0.01032978  0.45965004 -0.24445844 -0.55852365  0.24808168 -0.11205876\n",
      "   0.5105331   0.04520655 -0.23952252  0.5826858 ]\n",
      " [-0.19490597  0.16619009 -0.34416226  0.34050715 -0.07292521 -0.5085869\n",
      "  -0.21194148 -0.21187562  0.04639184  0.48566508]]; shape: (4, 10)\n",
      "First layer bias: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]; shape: (10,)\n",
      "Second layer weights: [[-0.35849732 -0.42499927  0.13113606]\n",
      " [ 0.549019    0.1469416   0.36308134]\n",
      " [-0.14331126  0.6500288   0.23577857]\n",
      " [-0.43616134  0.51825607 -0.04324549]\n",
      " [ 0.364645   -0.5676175   0.08043587]\n",
      " [ 0.41355872  0.5311277  -0.53330725]\n",
      " [-0.20217308 -0.37452525  0.11094409]\n",
      " [ 0.28362912  0.11707801  0.03815901]\n",
      " [ 0.31074506 -0.28761315 -0.08686286]\n",
      " [ 0.51159096  0.26241738 -0.07161206]]; shape: (10, 3)\n",
      "Second layer bias: [0. 0. 0.]; shape: (3,)\n",
      "number of layes: 3\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " Hidden_Layer (Dense)        (None, 10)                50        \n",
      "                                                                 \n",
      " Final_Layer (Dense)         (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83\n",
      "Trainable params: 83\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check model architecture and initial weights.\n",
    "W_1, b_1 = model.layers[1].get_weights()\n",
    "print(\"First layer weights: %s; shape: %s\" % (W_1,W_1.shape))\n",
    "print(\"First layer bias: %s; shape: %s\" % (b_1,b_1.shape))\n",
    "W_2, b_2 = model.layers[2].get_weights()\n",
    "print(\"Second layer weights: %s; shape: %s\" % (W_2,W_2.shape))\n",
    "print(\"Second layer bias: %s; shape: %s\" % (b_2,b_2.shape))\n",
    "print(\"number of layes: \" + str(len(model.layers)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk8x5Dy0ptX5"
   },
   "source": [
    "<b>Exercise 5:</b> Train the model on the toy data set generated below: \n",
    "\n",
    "Hints: \n",
    "\n",
    "* Keras expects one-hot-coded labels \n",
    "\n",
    "* Don't forget to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_4mu3twRptX6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2023-06-29 14:38:37.915611: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f681800bb10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-29 14:38:37.915682: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce MX130, Compute Capability 5.0\n",
      "2023-06-29 14:38:37.956178: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-29 14:38:40.654305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-29 14:38:40.690230: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:530] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.8\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2023-06-29 14:38:40.690522: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-06-29 14:38:40.691022: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-06-29 14:38:40.691086: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "\t [[{{node StatefulPartitionedCall_2}}]]\n",
      "2023-06-29 14:38:40.720667: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-06-29 14:38:40.721132: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-06-29 14:38:40.749123: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-06-29 14:38:40.749582: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-06-29 14:38:40.776954: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-06-29 14:38:40.777613: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_2' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3576/1626964001.py\", line 31, in <module>\n      history = model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), epochs=10, batch_size=32)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_2'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_2}}]] [Op:__inference_train_function_842]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_onehot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m##################################\u001b[39;00m\n",
      "File \u001b[0;32m~/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_2' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3576/1626964001.py\", line 31, in <module>\n      history = model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), epochs=10, batch_size=32)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/yuvi_dh/MSc_Life/Sem_two/uni_ml/venv/lib/python3.11/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_2'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_2}}]] [Op:__inference_train_function_842]"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = init_toy_data(1000,4,3, seed=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=67)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train_encoded.reshape(-1, 1))\n",
    "y_test_onehot = onehot_encoder.transform(y_test_encoded.reshape(-1, 1))\n",
    "\n",
    "##################################\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), epochs=10, batch_size=32)\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np35zqMPjrjo"
   },
   "source": [
    "Compare the test accuracy with the train accuracy. What can you see? Is the model performing well?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "NeuralNetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
