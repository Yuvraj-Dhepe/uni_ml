{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Yuvraj-Dhepe/uni_ml/blob/main/dida_task/Colab_Final_Version_of_Dida_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **P.S. On some runs, of this notebook I got poor segmentations by NN's on both normal and augmented images, maybe it depended on the Colab's GPU n Ram performance. So re-running a notebook is a good choice if you see a very poor segmentation in some run**\n",
    "- **You can directly go to [Observations sections](#Observations-Section) (Link works in local jupyter notebook) to look how my final submission plots look like**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LquPyZR4dLTT",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Brief About the Rooftop segmentation Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9zrIr41dLTX"
   },
   "source": [
    "Dear team,\n",
    "Thank you for allowing me to work on this task. It was pretty engaging and I got to try different aspects on how to handle a deep learning architecture with less data, and extract the best out of an algorithm. Before we get into codes, I thought to explain a bit of work I did, and how it's divided into the following sections. I have explained these sections in a bit more detail where I felt the explanation is necessary, you can find it at the beginning of each section. Also I have placed appropriate comments at various places in code to make the understanding effortless.\n",
    "\n",
    "**If you have any questions feel free to reachout on my [email](mailto:yuvi.kiit@gmail.com)**\n",
    "\n",
    "P.S.: **Execute collapsed sections is the best way to run the whole notebook in just 6 clicks and following hyperlinks to jump to respective sections**\n",
    "\n",
    "I divided the notebook into 5 sections:\n",
    "- [Importing and Installing the required Libraries Section](#Importing-and-Installing-the-required-Libraries-Section):\n",
    "    - Section consisting of all important library imports required to execute this notebook\n",
    "    \n",
    "- [Utility Functions & Variables Section](#Utility-Functions-&-Variables-Section):\n",
    "    - Includes function definitions (except network architecture defining functions) used for notebook code execution \n",
    "    \n",
    "- [Loading, Augmenting and Visualizing Images Section](#Loading,-Augmenting-and-Visualizing-Images-Section):\n",
    "    - Uses the utility preprocessing functions to load images, from respective folders\n",
    "    \n",
    "- [Model Training Section](#Model-Training-Section):\n",
    "    - This section includes model architecture defining functions, training these models, calculating metrics of tpr, fpr, on validation sets.\n",
    "\n",
    "- [Observations Section](#Observations-Section):\n",
    "    - In this section comprehensive comparision between models is done on base of their training, validation accuracy and loss plots, ROC Curve and evaluate them on metrics of precision, recall and f1 score\n",
    "\n",
    "- [Best Models Section](#Best-Models-Section):\n",
    "    - This section simply includes training of best models on whole training data both normal and augmented. After training visual prediction plots are generated with optimal threshold values obtained during model predictions on validation sets.\n",
    "\n",
    "- [References and Conculsions Section](#References-and-Conclusions-Section): \n",
    "    - Includes references I went through during this segmentation tasks, and conclusions I made from this whole task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wpm8tE8LQwA-",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading G-Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znz8tjqtlIPM",
    "outputId": "78b305b2-d07f-4eb7-c78e-5567e628652b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I8Xn6yulffV",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importing and Installing the required Libraries Section\n",
    "- [Link To Brief](#Brief-About-the-Rooftop-segmentation-Task)\n",
    "- [Link To Utility Functions & Variables Section](#Utility-Functions-&-Variables-Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ROSUj9GLHLo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "!pip install ipympl\n",
    "!apt remove git -y\n",
    "!apt-get install git -y && git clone https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKX8OdaAlL4v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import tensorflow as tf\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.spatial.distance import euclidean\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications import vgg16, MobileNetV2\n",
    "from examples.tensorflow_examples.models.pix2pix import pix2pix\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization,GlobalMaxPooling2D,\\\n",
    " GlobalAveragePooling2D, Conv2DTranspose, ZeroPadding2D, concatenate, Input, Activation, Dropout\n",
    "\n",
    "#import ipympl\n",
    "# Enables Use of Ipympl to zoom in and out of the plots\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# from google.colab import output\n",
    "# output.disable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAszbrEDJCp2",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Utility Functions & Variables Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co5qYkxadLTd"
   },
   "source": [
    "- [Link To Importing and Installing the required Libraries Section](#Importing-and-Installing-the-required-Libraries-Section)\n",
    "\n",
    "- Includes function definitions (except network architecture defining functions) used for notebook code execution \n",
    "    - [Preprocessing Functions](#Preprocessing-Functions): Functions to preprocess and augment images\n",
    "    - [Metric Functions](#Metric-Functions): Functions to generate ROC metrics, Cross Validation, F1, Recall and Precision scores\n",
    "    - [Visualization Functions](#Visualization-Functions): Functions to create a viz for ROC, Accuracy and Loss plots\n",
    "    - [Model Info Functions](#Model-Info-Function): Generates model architecture's dataframe and a diagramatic png representation\n",
    "\n",
    "- [Link To Loading, Augmenting and Visualizing Images Section](#Loading,-Augmenting-and-Visualizing-Images-Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVLDVtSKpZDj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Global Variables\n",
    "model_histories = {} # {model_name: training_history}\n",
    "models = {} # {model_name:[model,tprs,fprs]} Used for making prediction plots, ROC plots for all models at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd1amPZscNbB",
    "tags": []
   },
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LCpEsRkcSUr",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Image Loading and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1q_-083bcaY-"
   },
   "outputs": [],
   "source": [
    "def train_test_img_loader(path:str, size:int)-> np.array:\n",
    "  '''\n",
    "  Load images from folders\n",
    "  @param: path: directory path\n",
    "  @param: size: new size for images\n",
    "  '''\n",
    "  # Getting Names of images\n",
    "  image_files = sorted(os.listdir(path)) \n",
    "  # Blank array to store images\n",
    "  images = []\n",
    "  # Iterating through every image location, reading it and storing it in an array\n",
    "  for file in image_files:\n",
    "    image_path = os.path.join(path, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = resize(image, (size,size))\n",
    "    images.append(resized_image)\n",
    "  return np.array(images)\n",
    "\n",
    "def label_img_loader(path:str, size:int)-> np.array:\n",
    "  '''\n",
    "  Load labels from folders\n",
    "  @param: path: directory path\n",
    "  @param: size: new size for images\n",
    "  '''\n",
    "  # Getting Names of images\n",
    "  image_files = sorted(os.listdir(path)) \n",
    "  # Blank array to store images\n",
    "  images = []\n",
    "  # Iterating through every image location, reading it and storing it in an array\n",
    "  for file in image_files:\n",
    "    image_path = os.path.join(path, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_label = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "    resized_label = resize(gray_label, (size,size))\n",
    "    extended_image = tf.expand_dims(resized_label, axis = -1) # Grayscale channel\n",
    "    binary_image = np.where(extended_image>0.5,1,0) # Binary Masking for the pixels, going with general condition that any pixel value greater than 0.5 is considered as part of roof else not roof.\n",
    "    images.append(binary_image)\n",
    "  return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUPSNDD1cWr1",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Image Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdPbhIOqcoAU"
   },
   "outputs": [],
   "source": [
    "def augment_images(images: np.array, labels:np.array, num_aug:int=6) -> tuple():\n",
    "  '''\n",
    "  Generate augmented images, per given image, this function generates num_aug images\n",
    "  @param: images: Images to be augmented \n",
    "  @param: labels: Corresponding labels of images\n",
    "  @param: num_aug: Number of augmentations to produce for each image\n",
    "  '''\n",
    "\n",
    "  # Define the transforms to apply on both the images and the labels\n",
    "  transforms = A.Compose([\n",
    "      A.Resize(224, 224,interpolation = cv2.INTER_NEAREST), #Interpolation of image to preserve pixel values\n",
    "      A.HorizontalFlip(p=0.5),\n",
    "      A.VerticalFlip(p=0.5),\n",
    "      A.Rotate(limit=45, p=0.5),\n",
    "  ])\n",
    "\n",
    "  # Get the list of images and labels\n",
    "  images_list = images\n",
    "  labels_list = labels\n",
    "    \n",
    "  # Define empty numpy arrays to store the augmented images and labels, uint 8 is used for efficient processing of images\n",
    "  aug_image_list = np.empty((0,224,224,3),dtype = np.uint8)\n",
    "  aug_label_list = np.empty((0,224,224,1),dtype = np.uint8)\n",
    "\n",
    "  # Apply the transforms to each image and label pair and store them in corressponding arrays\n",
    "  for i in range(len(images_list)):\n",
    "    # Load the image and label and put it in our augmented list\n",
    "    image = images_list[i]\n",
    "    label = labels_list[i]\n",
    "    aug_image_list = np.append(aug_image_list,[image],axis = 0)\n",
    "    aug_label_list = np.append(aug_label_list,[label],axis = 0)\n",
    "\n",
    "    for j in range(num_aug):\n",
    "      # Apply the transforms to both the image and the label\n",
    "      transformed = transforms(image=image, mask=label)\n",
    "      image_transformed = transformed['image']\n",
    "      label_transformed = transformed['mask']\n",
    "      \n",
    "      aug_image_list = np.append(aug_image_list,[image_transformed],axis = 0)\n",
    "      aug_label_list = np.append(aug_label_list,[label_transformed],axis = 0)\n",
    "  \n",
    "  return aug_image_list, aug_label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hixWZMuzeLpg",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Return Images to model.fit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoRQhvUteJ6k"
   },
   "outputs": [],
   "source": [
    "def img(ip_type:str = 'augmented') -> tuple:\n",
    "  '''\n",
    "  Returns the images to the fit function\n",
    "  @param: ip_type: Image Set to be Returned either normal images or augmented images\n",
    "  '''\n",
    "  if ip_type == 'normal':\n",
    "    return processed_train_images,processed_train_labels,processed_val_images, processed_val_labels\n",
    "  elif ip_type== 'augmented':\n",
    "    return train_aug_images, train_aug_labels, val_aug_images, val_aug_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8fT28wpbHfF",
    "tags": []
   },
   "source": [
    "### Metric Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSgL-LaQb4GA",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Cross Validation Scores Generating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4B6fsuSkb8oX"
   },
   "outputs": [],
   "source": [
    "def cv(m:Model ,folds:int,dataset:str) -> None:\n",
    "  '''\n",
    "  Prints CV scores per fold and generate a viz, depicting the cv scores\n",
    "  @param: model: A trained neural network\n",
    "  @param: folds: Number of folds to divide the dataset\n",
    "  @param: dataset: Type of dataset on which cross validation has to be performed\n",
    "  '''\n",
    "  # Load your data\n",
    "  imgs = img(dataset)\n",
    "\n",
    "  X = imgs[0]\n",
    "  y = imgs[1]\n",
    "\n",
    "  # Define the K-Fold cross-validation iterator\n",
    "  kfold = KFold(n_splits=folds, shuffle=False, random_state=None)\n",
    "  scores =  []\n",
    "  fold_indices = []\n",
    "  \n",
    "  # Defining the splits indexes  \n",
    "  for train_index, val_index in kfold.split(X):\n",
    "      fold_indices.append((train_index, val_index))\n",
    "\n",
    "  # Perform cross-validation\n",
    "  for fold, (train_indices, val_indices) in enumerate(fold_indices):\n",
    "      model = m\n",
    "      print(f\"Fold {fold+1}:\")\n",
    "\n",
    "      # Split the data into training and validation sets\n",
    "      X_train, y_train = X[train_indices], y[train_indices]\n",
    "      X_val, y_val = X[val_indices], y[val_indices]\n",
    "\n",
    "      # Fit the model on the training set\n",
    "      model.fit(X_train, y_train, epochs=15, batch_size=5, validation_data=(X_val, y_val), verbose=0)\n",
    "      \n",
    "      # Evaluate the model on the validation set\n",
    "      score = model.evaluate(X_val, y_val, verbose=0)\n",
    "      scores.append(score)\n",
    "      print(f\"Validation accuracy: {score[1]*100}%\")\n",
    "      print(f\"Validation Loss: {score[0]*100}%\")\n",
    "\n",
    "  # Plot validation accuracy\n",
    "  # Generate x-axis tick labels as integers\n",
    "  scores = np.array(scores)\n",
    "  average_accuracy = np.mean(scores[0])\n",
    "  average_loss = np.mean(scores[1])\n",
    "  xticks = np.arange(scores.shape[0])\n",
    "  \n",
    "  print(f\"Average Validation accuracy: {average_accuracy}%\")\n",
    "  print(f\"Average Validation Loss: {average_loss}%\")\n",
    "    \n",
    "  #print(fold_indices)\n",
    "  # Plot the scores with integer xticks\n",
    "  plt.plot(xticks, scores[:, 1], label='Val Accuracy')\n",
    "  plt.plot(xticks, scores[:, 0], label='Val Loss')\n",
    "  plt.xticks(xticks)\n",
    "  plt.title('Validation Scores for Cross Validation')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAtYZ0ZWbKSA",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### ROC Curve, Threshold, F1-Score, Recall, Precision Generating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfvVn-SUbqci"
   },
   "outputs": [],
   "source": [
    "def joint_metrics(models:dict) -> None:\n",
    "  '''\n",
    "  Prints Precision, Recall and F1-Score for all models\n",
    "  @param: models: A dictionary of models consisting of trained model, it's tprs and fprs on validation data\n",
    "  '''  \n",
    "  for name,vals in models.items():\n",
    "    val_imgs = processed_val_images\n",
    "    val_labs = processed_val_labels\n",
    "    model_name = name\n",
    "    model = vals[0]\n",
    "    tprs = vals[1]\n",
    "    fprs = vals[2]\n",
    "    threshold = vals[3]\n",
    "    print(f\"{model_name} metrics\")\n",
    "    model_score(model,val_imgs,val_labs,threshold)\n",
    "    print(\"==================================================\")\n",
    "\n",
    "def model_score(model: Model,val_images: np.array,val_labels: np.array,thr:float) -> None:\n",
    "  '''\n",
    "  Prints the Precision, Recall and F1-Score for a given model\n",
    "  @param: model: A trained neural network\n",
    "  @param: val_images: The validation images to generate preds\n",
    "  @param: val_labels: The validation labels to compare with preds\n",
    "  @param: thr: Optimal threshold for a model, found from ROC curve\n",
    "  '''\n",
    "  # Get best threshold\n",
    "  threshold = thr\n",
    "\n",
    "  # Make predictions on validation set\n",
    "  predicted_images = model.predict(val_images,verbose = 0)\n",
    "\n",
    "  # Convert image to binary values depending on threshold\n",
    "  binary_preds = (predicted_images>threshold).astype('uint8')\n",
    "  binary_preds = np.reshape(binary_preds,(-1,224,224,1)) \n",
    "\n",
    "  # Calculate true positives, true negatives, false positives, and false negatives\n",
    "  cm = confusion_matrix(val_labels.flatten(),binary_preds.flatten())\n",
    "  tn,fp,fn,tp = cm.ravel()\n",
    "\n",
    "  # Calculate precision, recall, and F1 score\n",
    "  precision = tp/(tp+fp)\n",
    "  recall = tp/(tp+fn)\n",
    "  f1_score = 2*(precision*recall)/(precision+recall)\n",
    "  \n",
    "  # Print the results\n",
    "  print(f\"Precision: {precision}\")\n",
    "  print(f\"Recall: {recall}\")\n",
    "  print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "def roc_curve(model: Model,val_images: np.array,val_labels: np.array) -> tuple:\n",
    "  '''\n",
    "  Build an ROC curve for various thresholds based on validation set and return corresponding tprs and fprs\n",
    "  @param: model: A trained neural network\n",
    "  @param: val_images: The validation images to generate preds\n",
    "  @param: val_labels: The validation labels to compare with preds \n",
    "  '''\n",
    "  # Make predictions on validation set\n",
    "  predicted_images = model.predict(val_images,verbose = 0)\n",
    "\n",
    "  # Threshold the predicted images using different threshold values\n",
    "  thresholds = np.arange(0, 1.05, 0.05) # check all thresholds from 0-1 in 0.05 interval\n",
    "  tprs = []\n",
    "  fprs = []\n",
    "\n",
    "  for threshold in thresholds:\n",
    "      # apply threshold and convert to binary images\n",
    "      \n",
    "      binary_images = (predicted_images > threshold).astype('uint8')\n",
    "      binary_images = np.reshape(binary_images, (-1, 224, 224, 1))\n",
    "      \n",
    "      # calculate confusion matrix\n",
    "      cm = confusion_matrix(val_labels.flatten(), binary_images.flatten())\n",
    "      tn, fp, fn, tp = cm.ravel()\n",
    "      \n",
    "      # calculate TPR and FPR\n",
    "      tpr = tp / (tp + fn)\n",
    "      fpr = fp / (fp + tn)\n",
    "      \n",
    "      tprs.append(tpr)\n",
    "      fprs.append(fpr)\n",
    "\n",
    "  # plot the ROC curve\n",
    "  plt.plot(fprs, tprs)\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('ROC Curve')\n",
    "  plt.show()\n",
    "\n",
    "  return tprs,fprs\n",
    "\n",
    "\n",
    "def thr_calc(tprs:list, fprs:list) -> float:\n",
    "  '''\n",
    "  Calculate the best threshold value on base of TPRS and FPRS from ROC curve\n",
    "  @param: tprs: True Positive Rate\n",
    "  @param: fprs: False Positive Rate\n",
    "  '''\n",
    "  # Threshold the predicted images using different threshold values\n",
    "  thresholds = np.arange(0, 1.05, 0.05) # check all thresholds from 0-1 in 0.05 interval\n",
    "\n",
    "  # calculate distance to top left corner for each point on the ROC curve\n",
    "  distances = [euclidean([0, 1], [fprs[i], tprs[i]]) for i in range(len(fprs))]\n",
    "\n",
    "  # find index of point with smallest distance\n",
    "  best_index = np.argmin(distances)\n",
    "\n",
    "  # get corresponding threshold value which will be use to highlight the image\n",
    "  best_threshold = thresholds[best_index]\n",
    "  return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjloPmupPd7Y",
    "tags": []
   },
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta38fNi20ndq",
    "tags": []
   },
   "source": [
    "\n",
    "#### Prediction Viz Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zNnI0Lx0iJM"
   },
   "outputs": [],
   "source": [
    "def bst_joint_preds(models:dict,test_imgs: np.array) -> None:\n",
    "  '''\n",
    "  Visualize predictions on test images by best models\n",
    "  @param: models: A dictionary of models consisting of trained model, it's tprs and fprs on validation data\n",
    "  @param: test_imgs: Numpy array of test images\n",
    "  '''\n",
    "  for name,vals in models.items():\n",
    "    test_images = test_imgs\n",
    "    model_name = name\n",
    "    model = vals[0]\n",
    "    threshold = vals[1] \n",
    "    test_predictions(model,processed_test_images,threshold,model_name)\n",
    "        \n",
    "def joint_preds(models:dict,test_imgs:np.array) -> None:\n",
    "  '''\n",
    "  Visualize predictions on test images by all models\n",
    "  @param: models: A dictionary of models consisting of trained model, it's tprs and fprs on validation data\n",
    "  @param: test_imgs: Numpy array of test images\n",
    "  '''\n",
    "  for name,vals in models.items():\n",
    "    test_images = test_imgs\n",
    "    model_name = name\n",
    "    model = vals[0]\n",
    "    threshold = vals[3] \n",
    "  \n",
    "    test_predictions(model,processed_test_images,threshold,model_name)\n",
    "\n",
    "def test_predictions(model: Model,test_images: np.array,thr:float,model_name:str = None) -> None:\n",
    "  '''\n",
    "  Generate predictions on the test set\n",
    "  @param: model: A trained neural network\n",
    "  @param: test_images: Preprocessed Test Images\n",
    "  @param: tprs: True Positive rate, generated from ROC \n",
    "  @param: fprs: False Positive rate, generate from ROC \n",
    "  '''\n",
    "  # Get Best Threshold\n",
    "  best_threshold = thr\n",
    "  \n",
    "  # Generate predictions with the model\n",
    "  predicted_images = model.predict(test_images,verbose = 0) # predict on test images\n",
    "  \n",
    "  # apply threshold and convert to binary images\n",
    "  binary_images = (predicted_images > best_threshold).astype('uint8')\n",
    "  binary_images = np.reshape(binary_images, (-1, 224, 224, 1))\n",
    "  \n",
    "  \n",
    "  if model_name !=None:\n",
    "    print(f\" {model_name} predictions for threshold value: {best_threshold}\")\n",
    "  else: \n",
    "    print(f\"Predictions for best threshold value: {best_threshold}\")\n",
    "\n",
    "  fig, axs = plt.subplots(1,5,figsize = (18,18),sharey = True,dpi = 150)\n",
    "\n",
    "    # highlight rooftops in binary images\n",
    "  for i in range(binary_images.shape[0]):\n",
    "    test_image = test_images[i]\n",
    "    binary_image = binary_images[i].squeeze()\n",
    "    \n",
    "    # Create an image with only the rooftop pixels highlighted\n",
    "    rooftop_image = np.zeros_like(test_image)\n",
    "    rooftop_image[:,:,0] = binary_image*255 # set red channel to binary image\n",
    "    \n",
    "    # Blend the original image and the rooftop image to highlight rooftops\n",
    "    final_image = cv2.addWeighted(test_image, 0.8, rooftop_image, 0.2, 0)  # 80% contribution of test image and 20% contribution of predicted image\n",
    "  \n",
    "    # Adjusting the datarange to have no warnings with imshow  \n",
    "    final_image = np.clip(final_image,0,1)\n",
    "\n",
    "    axs[i].imshow(final_image)\n",
    "\n",
    "  # Giving an appropriate title \n",
    "  if model_name !=None:\n",
    "    plt.title(f\"{model_name} predictions\")\n",
    "  plt.show()\n",
    "  fig.tight_layout()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEQCZAw4MVzX",
    "tags": []
   },
   "source": [
    "#### Joint Plots Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyYPkAt9JCp4"
   },
   "outputs": [],
   "source": [
    "def joint_plots(model_histories:dict,epochs:int) -> None:\n",
    "  '''\n",
    "  Function to plot accuracy and loss from model histories, collected after every model training\n",
    "  @param: model_histories: Dict(key:model_name -> val: model_history)\n",
    "  @param: epochs: Number of epochs the models were trained on\n",
    "  '''\n",
    "  model_histories = model_histories\n",
    "  \n",
    "  # Null Dictionaries to store the history\n",
    "  train_loss = {}\n",
    "  train_acc = {}\n",
    "  val_loss = {}\n",
    "  val_acc = {}\n",
    "\n",
    "  # Storing Loss and Accuracy Metrics in appropriate dictionaries model wise\n",
    "  for model, history in model_histories.items():\n",
    "    train_loss[model] = history.history['loss'] \n",
    "    val_loss[model] = history.history['val_loss']\n",
    "    train_acc[model] = history.history['accuracy']\n",
    "    val_acc[model] = history.history['val_accuracy']\n",
    "\n",
    "  # Defining the figure, and a join dictionary consisting of individual category metrics.\n",
    "  fig,axs = plt.subplots(2,2,figsize = (18,12),dpi = 150)\n",
    "  categories = {'Val Accuracy':val_acc, 'Val Loss': val_loss,'Train Accuracy':train_acc,'Train Loss':train_loss,}\n",
    "\n",
    "  # Axes Logic for Plotting\n",
    "  # 1 -> 0,0\n",
    "  # 2 -> 0,1\n",
    "  # 3 -> 1,0\n",
    "  # 4 -> 1,1\n",
    "\n",
    "  l,m = 0,0\n",
    "  \n",
    "  # Going through every category metrics of val acc, val loss, train acc and train loss\n",
    "  for category in categories:\n",
    "    # In each category we iterate through a model and it's corresponding metric values.\n",
    "    for model, val in categories[category].items():\n",
    "      sns.lineplot(x = range(1,len(val)+1), y = val, label = model, ax = axs[l][m])\n",
    "    axs[l][m].set_xticks([i for i in range(1,epochs+1)])\n",
    "    axs[l][m].set_title(category)\n",
    "    axs[l][m].set_xlabel('Epochs')\n",
    "    axs[l][m].set_ylabel(category.split()[-1]) #Sets the label to Loss or Accuracy depending on category\n",
    "    m+=1\n",
    "    if m>1:\n",
    "      l = 1\n",
    "      m = 0\n",
    "\n",
    "  plt.show()\n",
    "  fig.tight_layout()\n",
    "\n",
    "def joint_roc(models:dict) -> None:\n",
    "  '''\n",
    "  Generate a ROC Curve for all the models in a single plot\n",
    "  @param: models: A dictionary of models consisting of trained model, it's tprs and fprs on validation data\n",
    "  '''\n",
    "  fig  = plt.figure(figsize=(18,6),dpi=150)\n",
    "  for name,vals in models.items():\n",
    "    test_images = processed_test_images\n",
    "    model_name = name\n",
    "    model = vals[0]\n",
    "    tprs = vals[1]\n",
    "    fprs = vals[2]\n",
    "    plt.plot(fprs,tprs,label = model_name)\n",
    "  \n",
    "  plt.title(f'Combined ROC')\n",
    "  plt.legend()\n",
    "  plt.xlabel('FPR')\n",
    "  plt.ylabel('TPR')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvT8ObIqdtTK",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Single Model Metrics Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FejkNwxkdqe4"
   },
   "outputs": [],
   "source": [
    "def acc_loss_plot(history:list):\n",
    "  '''\n",
    "  Function that plots accuracy and loss metrics from an Models History\n",
    "  @param: history : Any model History\n",
    "  '''\n",
    "  # Get the loss from history object\n",
    "  train_loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  train_acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  \n",
    "  # create figure and axis objects\n",
    "  fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8, 8),dpi = 150)\n",
    "\n",
    "  # plot the training and validation loss\n",
    "  sns.lineplot(x=range(len(train_loss)), y=train_loss, ax=ax[0], label='train')\n",
    "  sns.lineplot(x=range(len(val_loss)), y=val_loss, ax=ax[0], label='val')\n",
    "  ax[0].set_title('Loss')\n",
    "  ax[0].set_xlabel('Epoch')\n",
    "  ax[0].set_ylabel('Loss')\n",
    "\n",
    "  # plot the training and validation accuracy\n",
    "  sns.lineplot(x=range(len(train_acc)), y=train_acc, ax=ax[1], label='train')\n",
    "  sns.lineplot(x=range(len(val_acc)), y=val_acc, ax=ax[1], label='val')\n",
    "  ax[1].set_title('Accuracy')\n",
    "  ax[1].set_xlabel('Epoch')\n",
    "  ax[1].set_ylabel('Accuracy')\n",
    "\n",
    "  # show the plot\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko9iIW6GJCp5",
    "tags": []
   },
   "source": [
    "### Model Info Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahPjaMiWJCp5"
   },
   "outputs": [],
   "source": [
    "def model_info(model: Model):\n",
    "  '''\n",
    "  Returns the whole architecture information for a model\n",
    "  @param: model: A neural network architecture\n",
    "  '''\n",
    "  plot_model(model, to_file=f'{model.name}.png', show_shapes=True, rankdir='TB')\n",
    "  layers = [(layer, layer.name, layer.trainable,layer.output.shape) for layer in model.layers]\n",
    "  df = pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable', 'Layer Op'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR0xMSJKP4DC",
    "tags": []
   },
   "source": [
    "## Loading, Augmenting and Visualizing Images Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16CzX5WUdLTj"
   },
   "source": [
    "- [Link To Utility Functions & Variables Section](#Utility-Functions-&-Variables-Section)\n",
    "\n",
    "- Uses the utility preprocessing functions to load images, from respective folders\n",
    "    - Points to note:\n",
    "        - Images were renamed from (1 to 25 for train) just for simplicity\n",
    "        - 25 Training images were divided into 20 train and 5 validation images manually for model evaluation on validation images, keeping in mostly used train_val 75:25 split\n",
    "            - [Images Folder Link](https://drive.google.com/drive/folders/1U-WStjfOKd1KKodxgmTAa-Xd4T6CdenQ?usp=share_link) after renaming and making train val split\n",
    "            - **However in the best model training I use the complete train set of 25 images**\n",
    "        - The label images were converted to binary images by using a threshold of 0.5 (since it's usual for black and white images and using any other threshold, doesn't do much to performance of model)\n",
    "        - Images were resized to (224,224,3) for train and (224,224,1) for labels, main reason being to make images suitable for NN architectures\n",
    "        - Since given dataset was pretty small, 6 augmentations were produced per image and label, by using albumentations library.\n",
    "            - The main reason to use external library instead of widely used tf's DataLoaders , cause after augmentations, few images didn't match with it's respective augmented version\n",
    "\n",
    "- [Link To Model Training Section](#Model-Training-Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loZc4S3udLTj",
    "outputId": "c3e70f2b-e06c-4656-dfac-02011b074cff"
   },
   "outputs": [],
   "source": [
    "# Directory consists where there are train and test images\n",
    "%cd /content/gdrive/MyDrive/images/Dida_task/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdZQxLQ7c3ap",
    "tags": []
   },
   "source": [
    "#### Function to change Image Numbers for better tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOH5U9OxdAE7"
   },
   "outputs": [],
   "source": [
    "# # Set the path to your train directories\n",
    "# train_image_path = './train/images'\n",
    "# train_label_path = './train/labels'\n",
    "\n",
    "# # Rename the image files\n",
    "# image_files = sorted(os.listdir(train_image_path))\n",
    "# for i, file in enumerate(image_files):\n",
    "#     src = os.path.join(train_image_path, file)\n",
    "#     dst = os.path.join(train_image_path, f\"{i+1}.png\")\n",
    "#     os.rename(src, dst)\n",
    "\n",
    "# # Rename the label files\n",
    "# label_files = sorted(os.listdir(train_label_path))\n",
    "# for i, file in enumerate(label_files):\n",
    "#     src = os.path.join(train_label_path, file)\n",
    "#     dst = os.path.join(train_label_path, f\"{i+1}.png\")\n",
    "#     os.rename(src, dst)\n",
    "\n",
    "# # After renaming the images I took randomly 5 images and corresponding labels and created a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n64jFX78JCp6",
    "tags": []
   },
   "source": [
    "### Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YvqzY0opnw6"
   },
   "outputs": [],
   "source": [
    "# Directories of images\n",
    "train_image_path = './train/images/'\n",
    "train_label_path = './train/labels/'\n",
    "val_image_path = './val/images/'\n",
    "val_label_path = './val/labels/'\n",
    "test_image_path = './test/images/'\n",
    "\n",
    "# Loading the images from dirs\n",
    "# We are choosing this size of 224 so that Maxpooling works well\n",
    "train_images = train_test_img_loader(train_image_path,224) \n",
    "val_images = train_test_img_loader(val_image_path,224)\n",
    "train_labels = label_img_loader(train_label_path,224)\n",
    "val_labels = label_img_loader(val_label_path,224)\n",
    "test_images = train_test_img_loader(test_image_path,224)\n",
    "\n",
    "# Normalizing the images and making label images having enough channels for NN.\n",
    "processed_train_images = train_images.astype('float32')\n",
    "processed_val_images = val_images.astype('float32')\n",
    "processed_test_images = test_images.astype('float32')\n",
    "processed_train_labels = train_labels\n",
    "processed_val_labels = val_labels\n",
    "\n",
    "# Augmenting the images, generating 6 augmentations per image\n",
    "# Chosen 6 as lower than that decreases performance and higher than that overfits the model\n",
    "train_aug_images, train_aug_labels = augment_images(processed_train_images,processed_train_labels,num_aug = 6)\n",
    "val_aug_images, val_aug_labels = augment_images(processed_val_images,processed_val_labels,num_aug = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOrmrM14KQAS",
    "outputId": "bd04e667-77cf-469e-be1b-0ce560d1f4e3"
   },
   "outputs": [],
   "source": [
    "# Checking the shape of images\n",
    "#train_labels.shape,processed_train_images.shape,processed_val_images.shape,processed_val_labels.shape\n",
    "train_aug_images.shape, train_aug_labels.shape, val_aug_images.shape, val_aug_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8iUXQZuQMZ4"
   },
   "source": [
    "### Viewing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "kM-2BVxQnG-j",
    "outputId": "03d25c8b-b1f7-40d2-b881-982123e789d7"
   },
   "outputs": [],
   "source": [
    "# Seeing the augmented images for an original image with any index between 1 to 20\n",
    "idx = 15\n",
    "st = 7*(idx)\n",
    "fig, axs = plt.subplots(2, 7,figsize = (15,4))\n",
    "\n",
    "for i,j in enumerate(range(st,st+7)):\n",
    "  axs[0,i].imshow(train_aug_images[j])\n",
    "  axs[1,i].imshow(train_aug_labels[j].squeeze(), cmap='gray')\n",
    "fig.tight_layout()\n",
    "#axs[1,7].imshow(processed_test_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "LJk5Evjlq7mP",
    "outputId": "acde6614-1e59-4d6e-bff5-cb85b3c799c2"
   },
   "outputs": [],
   "source": [
    "# Seeing the original image and corresponding label\n",
    "idx = 15\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(processed_train_images[idx])\n",
    "axs[1].imshow(processed_train_labels[idx].squeeze(), cmap='gray')\n",
    "axs[2].imshow(processed_test_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFuWIGHaOQNc",
    "tags": []
   },
   "source": [
    "## Model Training Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lTeLpQ-16F6"
   },
   "source": [
    "- [Link To Loading, Augmenting and Visualizing Images Section](#Loading,-Augmenting-and-Visualizing-Images-Section)\n",
    "\n",
    "- This section includes model architecture defining functions, training these models, calculating metrics of tpr, fpr, thresholds on validation sets, and storing important bits such as thresholds, trained architecture etc.\n",
    "    - Model histories dictionary is used stores every models training, validation accuracy and losses.\n",
    "    - Models dictionary stores, trained models, with it's metrics of tprs, fprs and threshold calculated on validation set.\n",
    "    - Model Types Utilized: \n",
    "        - Models trained from Scratch: Used as base comparisions\n",
    "            - [Base Cnn Model](#Base-CNN-Model:-Trained-From-Scratch): Convolutiona neural network, Extended with decoders \n",
    "            - [Base Unet Model](#Base-UNET:-Trained-From-Scratch): U-NET architecture implementation\n",
    "        - Models built with Transfer Learning: \n",
    "            - [Pretrained VGG16](#Fine-Tuned-VGG16,-Combined-with-Unet): PowerFul feature extractor VGG16 extended with decoders, concatenated with conv blocks from VGG 16, similar to UNET.\n",
    "            - [Pretrained MobileNet](#Fine-Tuned-MobileNet,-Combined-with-Unet): Light n efficient feature extractor MobileNetV2 extended with decoders and concatenated with conv blocks from MobileNet, similar to UNET\n",
    "    - Every model is trained and validated on both Normal (non-augmented) and Augmented set of images, just for exploration and model comparisions.\n",
    "        - I have **provided notes on why I followed a certain architecture blocks or certain params** near the architecture defining functions, or in comments, wherever I felt it's useful.\n",
    "\n",
    "- The **models perform binary classification of the pixels** whether it's a part of roof or not, given an optimal threshold\n",
    "- The codes which are commented except of cross validation, will be run in observations part to compare all models at one place\n",
    "- Cross Validation wasn't that helpful, except providing information average performance on model from all folds, and performance on individual folds\n",
    "\n",
    "- **Only CNN and Unets ROC curve seem to be good, but not of vgg and mobilenet ones. The main reason I have discussed in observations section**\n",
    "\n",
    "- [Link To Observations Section](#Observations-Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KwGoZkI7y0JV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Fit parameters to be change all at once\n",
    "epochs = 20\n",
    "batch_size = 5\n",
    "# I found these values, by simple obervations of few training runs, and these values learn as much as is possible, trying to also not overfit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwKFUCn_QUgm",
    "tags": []
   },
   "source": [
    "### Base CNN Model: Trained From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For any image task, the base architecture that comes in mind to use is a CNN architecture\n",
    "- I am using CNN as the general base\n",
    "- For segmentation task, I combined basic convolutional layers and transpose layers to perform segmentation on the images, with the basis of labels.\n",
    "- I didn't made this architecture much deep since I am using it only as a base comparision\n",
    "- This simple architecture does a pretty good job with decreasing loss, but in comparision with other models it's not able to learn features from images accurately\n",
    "- Overall I think all the rest models performed better than CNN just cause they were able to extract more features from images, and had a skip layer concatenation, which improved segmentation in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-FbQ2OwMOt1"
   },
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def Cnn(input_size=(224, 224, 3), n_filters=16, n_classes=1, dropout_prob=0.5):\n",
    "    '''\n",
    "    Returns a CNN architecture\n",
    "    @param: input_size: input size of image\n",
    "    @param: n_filters: Number of filters to be used in architecture layers\n",
    "    @param: n_classes: Total Classes to be predicted\n",
    "    @param: dropout_prob: Dropout probabilities\n",
    "    '''\n",
    "    input_shape = input_size\n",
    "    model = Sequential(name='CNN')\n",
    "    \n",
    "    # Dropouts are used to avoid overfitting \n",
    "    \n",
    "    # Downsampling an image by conv layers to extract feature maps from images\n",
    "    model.add(Conv2D(n_filters*2, (5, 5), activation='relu', input_shape=input_shape, padding='valid'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    model.add(Conv2D(n_filters*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    model.add(Conv2D(n_filters*8, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    model.add(Conv2D(n_filters*16, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "    # Upsampling process to recover the spatial details and enhance the resolution of the feature maps to produce more accurate outputs.\n",
    "    model.add(Conv2DTranspose(n_filters*8, (4, 4), strides=(2, 2), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    model.add(Conv2DTranspose(n_filters*4, (4, 4), strides=(2, 2), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    model.add(Conv2DTranspose(n_filters*2, (4, 4), strides=(2, 2), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    model.add(Conv2D(n_filters*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_prob))\n",
    "    \n",
    "    # This layer produces the final image output, representing the predicted probabilities for each pixel.\n",
    "    # The sigmoid activation function is used to ensure the output values are between 0 and 1, representing the class probabilities.\n",
    "    model.add(Conv2D(n_classes, (3, 3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "    # Adding padding to get an image of the same size as the label\n",
    "    model.add(ZeroPadding2D(padding=((4, 4), (4, 4))))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gPFcy2CJCp8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Loading and training the CNN model without augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmNiZnwJtei6",
    "outputId": "265b6a4e-5409-491f-9777-6729d831ddcd"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "cnn = Cnn()\n",
    "\n",
    "# Get the model info\n",
    "## model_info(cnn)\n",
    "\n",
    "# Getting images\n",
    "imgs = img('normal')\n",
    "\n",
    "# Train the model\n",
    "h_cnn = cnn.fit(imgs[0],imgs[1],epochs=epochs, batch_size=batch_size, validation_data = (imgs[2],imgs[3]))\n",
    "\n",
    "# Storing the history \n",
    "model_histories['cnn'] = h_cnn\n",
    "\n",
    "# Plotting the accuracy and loss for train and validation data\n",
    "# We see all plots together at the end\n",
    "# acc_loss_plot(h_cnn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "w52ss_sfL_zZ",
    "outputId": "9f17ff15-aa52-485c-e6e0-7c058b80fc43"
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "tprs,fprs = roc_curve(cnn,processed_val_images,processed_val_labels)\n",
    "\n",
    "# Get best threshold on base of tprs and fprs\n",
    "threshold = thr_calc(tprs,fprs)\n",
    "\n",
    "# Storing the model, tprs, fprs\n",
    "models['cnn'] = [cnn,tprs,fprs,threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqNzgTaPWxHq"
   },
   "outputs": [],
   "source": [
    "# Generating model scores on validation images with best threshold\n",
    "# model_score(cnn,processed_val_images,processed_val_labels,tprs,fprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPOD0DrGjwvN"
   },
   "outputs": [],
   "source": [
    "# Predictions on the test set by getting the best threshold from the ROC tprs, fprs values\n",
    "# We do all predictions at the end.\n",
    "# test_predictions(cnn,processed_test_images,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeGMcd4wRAYt"
   },
   "outputs": [],
   "source": [
    "# # Observing the results of 5-fold Cross_Validation with Normal Images\n",
    "# cv_cnn = Cnn()\n",
    "# cv(m = cv_cnn, folds = 5,dataset = 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmT2UTjzJCp9",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Loading and training the CNN model with augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUV_bjC0JCp9",
    "outputId": "49612d8e-e683-4812-e259-8d332c0ec5e0"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "aug_cnn = Cnn()\n",
    "\n",
    "# Getting images\n",
    "imgs = img('augmented')\n",
    "\n",
    "# Train the model\n",
    "h_aug_cnn = aug_cnn.fit(imgs[0],imgs[1],epochs=epochs, batch_size=batch_size, validation_data = (imgs[2],imgs[3]))\n",
    "\n",
    "# Storing the history\n",
    "model_histories['aug_cnn'] = h_aug_cnn\n",
    "# Plotting the accuracy and loss for train and validation data\n",
    "# acc_loss_plot(h_aug_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "en9AtZRVJCp-",
    "outputId": "dde449f6-33fa-4eee-e2db-80b96856efcf"
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "tprs,fprs = roc_curve(aug_cnn,processed_val_images,processed_val_labels)\n",
    "\n",
    "# Get best threshold on base of tprs and fprs\n",
    "threshold = thr_calc(tprs,fprs)\n",
    "\n",
    "# Storing the model, tprs, fprs\n",
    "models['aug_cnn'] = [aug_cnn,tprs,fprs,threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FE6k6OFEaizB"
   },
   "outputs": [],
   "source": [
    "# Generating model scores on validation images with best threshold\n",
    "# model_score(aug_cnn,processed_val_images,processed_val_labels,tprs,fprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SX6TA9tzJCp-"
   },
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "# test_predictions(aug_cnn,processed_test_images,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9rgFPheBQ7L"
   },
   "outputs": [],
   "source": [
    "# # Observing the results of 5-fold Cross_Validation with Augmented Images\n",
    "# cv_cnn = Cnn()\n",
    "# cv(m = aug_cnn, folds = 5,dataset = 'augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCc7qn2xjr1y",
    "tags": []
   },
   "source": [
    "### Base UNET: Trained From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unet is an architecture which is specially developed for segmentatoin tasks\n",
    "- I am using this model as a base especially segmentation task, like how complex models do in segmenting an image wrt a simple UNET\n",
    "- The results are quite interesting when comparing UNET with pretrained models\n",
    "    - Complex models do a good job at only getting a bit better wrt accuracy of marking a rooftop better\n",
    "    - Whereas in terms of loss, they sometimes cross the loss of UNET \n",
    "- On Augmentation, the segmentation improves on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOeimh3C0Bpb"
   },
   "outputs": [],
   "source": [
    "def EncoderMiniBlock(inputs, n_filters:int=32, dropout_prob:float=0.3, max_pooling:bool=True):\n",
    "  '''\n",
    "  Return the encoding blocks to UNET architecture\n",
    "  @param: inputs:  A prev layer architecture\n",
    "  @param: n_filters: Number of filters to be used in conv layers\n",
    "  @param: dropout_prob: Dropout Probabilities\n",
    "  @param: max_pooling: Max pooling addition to encoder block\n",
    "  '''\n",
    "  conv = Conv2D(n_filters, (3,3), activation='relu', padding='same', kernel_initializer='HeNormal')(inputs)\n",
    "  conv = Conv2D(n_filters, (3,3), activation='relu', padding='same', kernel_initializer='HeNormal')(conv)\n",
    "  conv = BatchNormalization()(conv, training=False)\n",
    "  \n",
    "  if dropout_prob > 0:     \n",
    "      conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "  if max_pooling:\n",
    "      next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
    "  else:\n",
    "      next_layer = conv\n",
    "  \n",
    "  skip_connection = conv    \n",
    "  return next_layer, skip_connection\n",
    "\n",
    "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters:int=32):\n",
    "  '''\n",
    "  Return the encoding blocks to UNET architecture\n",
    "  @param: prev_layer_input:  A prev layer architecture\n",
    "  @param: skip_layer_input: Conv Layers to be linked to Transpose layer\n",
    "  @param: n_filtes: Number of filters to be used in transpose and conv layers\n",
    "  '''\n",
    "  up = Conv2DTranspose(n_filters, (3,3), strides=(2,2), padding='same')(prev_layer_input)\n",
    "  merge = concatenate([up, skip_layer_input],axis = 3)\n",
    "  conv = Conv2D(n_filters, (3,3), activation='relu', padding='same', kernel_initializer='HeNormal')(merge)\n",
    "  conv = Conv2D(n_filters, (3,3), activation='relu', padding='same', kernel_initializer='HeNormal')(conv)\n",
    "  return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "No4LrPOZ1VLP"
   },
   "outputs": [],
   "source": [
    "# Defining the UNET using the encoder and decoder functions\n",
    "def Unet(input_size=(224, 224, 3), n_filters=16, n_classes=1,dropout_prob = 0):\n",
    "  '''\n",
    "  Returns a Unet Architecture Model\n",
    "  @param: input_size: input size of image\n",
    "  @param: n_filters: Number of filters to be used in architecture layers\n",
    "  @param: n_classes: Total Classes to be predicted\n",
    "  @param: dropout_prob: Dropout probabilities\n",
    "  '''\n",
    "  # Input size represent the size of 1 image (the size used for pre-processing)\n",
    "  inputs = Input(input_size)\n",
    "\n",
    "  # Encoder includes multiple convolutional mini blocks with different maxpooling, dropout and filter parameters\n",
    "  # keeping dropout to be 0 because we have small dataset itself\n",
    "  # We increase n_filters per layer to extract more features\n",
    "  cblock1 = EncoderMiniBlock(inputs, n_filters,dropout_prob=0, max_pooling=True)\n",
    "  cblock2 = EncoderMiniBlock(cblock1[0],n_filters*2,dropout_prob=0, max_pooling=True)\n",
    "  cblock3 = EncoderMiniBlock(cblock2[0], n_filters*4,dropout_prob=0, max_pooling=True)\n",
    "  cblock4 = EncoderMiniBlock(cblock3[0], n_filters*8,dropout_prob=dropout_prob, max_pooling=True)\n",
    "  cblock5 = EncoderMiniBlock(cblock4[0], n_filters*16, dropout_prob=dropout_prob, max_pooling=False) \n",
    "\n",
    "  # Decoder includes multiple mini blocks with decreasing number of filters\n",
    "  # Skip connections from the encoder are given as input to the decoder\n",
    "  # Recall the 2nd output of encoder block was skip connection, hence cblockn[1] is used\n",
    "  ublock6 = DecoderMiniBlock(cblock5[0], cblock4[1],  n_filters * 8)\n",
    "  ublock7 = DecoderMiniBlock(ublock6, cblock3[1],  n_filters * 4)\n",
    "  ublock8 = DecoderMiniBlock(ublock7, cblock2[1],  n_filters * 2)\n",
    "  ublock9 = DecoderMiniBlock(ublock8, cblock1[1],  n_filters)\n",
    "\n",
    "  # Complete the model with 1 3x3 convolution layer (Same as the prev Conv Layers)\n",
    "  # Followed by a 1x1 Conv layer to get the image to the desired size. \n",
    "  # Observe the number of channels will be equal to number of output classes\n",
    "  conv9 = Conv2D(n_filters, (3,3), activation='relu', padding='same', kernel_initializer='he_normal')(ublock9)\n",
    "\n",
    "  conv10 = Conv2D(n_classes, (1,1), padding='same', activation = 'sigmoid')(conv9)\n",
    "  # This layer produces the final image output, representing the predicted probabilities for each pixel.\n",
    "  # The sigmoid activation function is used to ensure the output values are between 0 and 1, representing the class probabilities.\n",
    "\n",
    "  # Define the model\n",
    "  model = Model(inputs=inputs, outputs=conv10,name = 'UNET')\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP2b3S8pJCp_",
    "tags": []
   },
   "source": [
    "#### Loading and training the Unet model without augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV7FKFsP2aTE",
    "outputId": "959ac7c3-b65e-4433-c4e4-233c441ab42e"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "unet = Unet()\n",
    "\n",
    "# Getting images\n",
    "imgs = img('normal')\n",
    "\n",
    "#Train the model\n",
    "h_unet = unet.fit(imgs[0],imgs[1],epochs=epochs, batch_size=batch_size, validation_data = (imgs[2],imgs[3]))\n",
    "\n",
    "# Storing the history\n",
    "model_histories['unet'] = h_unet\n",
    "\n",
    "# Plotting the accuracy and loss for train and validation data\n",
    "# acc_loss_plot(h_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "k3bo_WIuJCqA",
    "outputId": "7b2fe7a9-ce9e-40c6-f6ca-c283931fa28e"
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "tprs,fprs = roc_curve(unet,processed_val_images,processed_val_labels)\n",
    "\n",
    "# Get best threshold on base of tprs and fprs\n",
    "threshold = thr_calc(tprs,fprs)\n",
    "\n",
    "# Storing the model, tprs, fprs\n",
    "models['unet'] = [unet,tprs,fprs,threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9cOfHNYb_Wk"
   },
   "outputs": [],
   "source": [
    "#  # Generating model scores on validation images with best threshold\n",
    "#model_score(unet,processed_val_images,processed_val_labels,tprs,fprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93GvVERDJCqA"
   },
   "outputs": [],
   "source": [
    "# # # Predictions on the test set\n",
    "# test_predictions(unet,processed_test_images,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFFCMQSmRlKk"
   },
   "outputs": [],
   "source": [
    "# # Observing the results of 5-fold Cross_Validation with Normal Images\n",
    "# cv_unet = Unet()\n",
    "# cv(m = cv_unet, folds = 5, dataset = 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBiKg7PkJCqA",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Loading and training the Unet model with augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYh9GTMCJCqA",
    "outputId": "16790465-37e8-4ae1-b521-b68ba968928e"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "aug_unet = Unet()\n",
    "\n",
    "# Getting images\n",
    "imgs = img('augmented')\n",
    "\n",
    "# Train the model\n",
    "h_aug_unet = aug_unet.fit(imgs[0],imgs[1],epochs=epochs, batch_size=batch_size, validation_data = (imgs[2],imgs[3]))\n",
    "\n",
    "# Storing the history\n",
    "model_histories['aug_unet'] = h_aug_unet\n",
    "\n",
    "# Plotting the accuracy and loss for train and validation data\n",
    "# acc_loss_plot(h_aug_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "5H0c3DGZJCqB",
    "outputId": "5b218d02-a39a-49f9-c299-7f26b4c44f75"
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "tprs,fprs = roc_curve(aug_unet,processed_val_images,processed_val_labels)\n",
    "\n",
    "# Get best threshold on base of tprs and fprs\n",
    "threshold = thr_calc(tprs,fprs)\n",
    "\n",
    "# Storing the model, tprs, fprs\n",
    "models['aug_unet'] = [aug_unet,tprs,fprs,threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbmI0MR5cOAk"
   },
   "outputs": [],
   "source": [
    "# # Generating model scores on validation images with best threshold\n",
    "#model_score(aug_unet,processed_val_images,processed_val_labels,tprs,fprs,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjigjHh_JCqB"
   },
   "outputs": [],
   "source": [
    "# # Predictions on the test set\n",
    "# test_predictions(aug_unet,processed_test_images,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xt8T14eNR3nk"
   },
   "outputs": [],
   "source": [
    "# # Observing the results of 5-fold Cross_Validation with Augmented Images\n",
    "# cv_unet = Unet()\n",
    "# cv(m = cv_unet, folds = 5,dataset = 'augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2laXwHDO-Wv_",
    "tags": []
   },
   "source": [
    "### Fine Tuned VGG16, Combined with Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VGG16, was my first choice with transfer learning, given it's powerful feature extraction with the pretrained waits of initial layers\n",
    "    - This helps to recover good features from the images, to be later used in skip layers in decoders\n",
    "- I trained the conv5 layer of VGG16, to adapt model to problem at hand and this architecture was extended by combining it with UNET Decoders, to do proper segmentation of images\n",
    "- The results in comparision to UNET were better in terms of mapping corners of rooftops especially when augmented images were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbtRUUCTIZii"
   },
   "outputs": [],
   "source": [
    "# Convolution Architecture layers name for VGG16, used in VGG_Unet architecture\n",
    "conv1 = ['block1_conv1','block1_conv2']\n",
    "conv2 = ['block2_conv1','block2_conv2']\n",
    "conv3 = ['block3_conv1','block3_conv2','block3_conv3']\n",
    "conv4 = ['block4_conv1','block4_conv2','block4_conv3']\n",
    "conv5 = ['block5_conv1','block5_conv2','block5_conv3']\n",
    "pool1 = ['block1_pool']\n",
    "pool2 = ['block2_pool']\n",
    "pool3 = ['block3_pool']\n",
    "pool4 = ['block4_pool']\n",
    "pool5 = ['block5_pool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAn1cw2MAXa4"
   },
   "outputs": [],
   "source": [
    "def Vgg_Unet(input_size=(224, 224, 3), n_filters=16, n_classes=1,dropout_prob = 0):\n",
    "  '''\n",
    "  Returns a VGG Unet Architecture Model\n",
    "  @param: input_size: input size of image\n",
    "  @param: n_filters: Number of filters to be used in architecture layers\n",
    "  @param: n_classes: Total Classes to be predicted\n",
    "  @param: dropout_prob: Dropout probabilities\n",
    "  '''\n",
    "\n",
    "  # Loading a pretrained VGG 16 model\n",
    "  VGG16 = vgg16.VGG16(include_top = False, weights = 'imagenet',input_shape = input_size)\n",
    "  last_layer = VGG16.output\n",
    "\n",
    "  # Defining which layers to train and which not to, since we have less images it's a good strategy to freeze the initial layers and only fine tune the last layer.\n",
    "  train_layers =  conv5 \n",
    "  non_train_layers = ['input_21'] + pool1 + pool2 + pool3 + pool4 + pool5+conv1 + conv2  + conv3 + conv4\n",
    "\n",
    "  for layer in VGG16.layers:\n",
    "    if layer.name in train_layers:\n",
    "      layer.trainable = True\n",
    "    if layer.name in non_train_layers:\n",
    "      layer.trainable = False\n",
    "  \n",
    "  # Extending the VGG16 via unet's decoder block, similar to UNET Decoder Architecture Explanation.\n",
    "  last_layer = VGG16.output\n",
    "  unet = DecoderMiniBlock(last_layer,VGG16.get_layer(\"block5_conv3\").output, n_filters*32)\n",
    "  unet = DecoderMiniBlock(unet,VGG16.get_layer(\"block4_conv3\").output, n_filters*32)\n",
    "  unet = DecoderMiniBlock(unet,VGG16.get_layer(\"block3_conv3\").output, n_filters*16)\n",
    "  unet = DecoderMiniBlock(unet,VGG16.get_layer(\"block2_conv2\").output, n_filters*8)\n",
    "  unet = DecoderMiniBlock(unet,VGG16.get_layer(\"block1_conv2\").output, n_filters*4)\n",
    "\n",
    "  # Adding few final conv layers to get image output with the same size as that of the labels\n",
    "  conv9 = Conv2D(n_filters*2, (3,3), activation='relu', padding='same', kernel_initializer='he_normal')(unet)\n",
    "  conv10 = Conv2D(n_filters, (3,3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "  conv11 = Conv2D(n_classes, (1,1), activation='sigmoid', padding='same', kernel_initializer='he_normal')(conv10)\n",
    "  # This layer produces the final image output, representing the predicted probabilities for each pixel.\n",
    "  # The sigmoid activation function is used to ensure the output values are between 0 and 1, representing the class probabilities.\n",
    "\n",
    "  model = Model(VGG16.input, conv11)\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qizGuVOJCqC",
    "tags": []
   },
   "source": [
    "#### Loading and training the VGG combined with UNET model without augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1zQpq0TAXzf",
    "outputId": "141f827d-fbc5-4ede-d3a3-b44d1ad570f4"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "vgg_unet = Vgg_Unet()\n",
    "\n",
    "# Getting images\n",
    "imgs = img('normal')\n",
    "\n",
    "# Train the model\n",
    "h_vgg_unet = vgg_unet.fit(imgs[0],imgs[1],epochs=epochs, batch_size=batch_size, validation_data = (imgs[2],imgs[3]))\n",
    "\n",
    "# Storing the history\n",
    "model_histories['vgg_unet'] = h_vgg_unet\n",
    "# Plotting the accuracy and loss for train and validation data\n",
    "# acc_loss_plot(h_vgg_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c08ExTNOAx3"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(vgg_unet, to_file='UNET with VGG16.png', show_shapes=True, rankdir='TB')\n",
    "# layers = [(layer, layer.name, layer.trainable,layer.output.shape) for layer in vgg_unet.layers]\n",
    "# pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable', 'Layer Op'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "yhgWAgohxV0O",
    "outputId": "107675ec-a07e-4429-e966-c494b0003d5c"
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "tprs,fprs = roc_curve(vgg_unet,processed_val_images,processed_val_labels)\n",
    "\n",
    "# Get best threshold on base of tprs and fprs\n",
    "threshold = thr_calc(tprs,fprs)\n",
    "\n",
    "# Storing the model, tprs, fprs\n",
    "models['vgg_unet'] = [vgg_unet,tprs,fprs,threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8ZPZm0VcwaU"
   },
   "outputs": [],
   "source": [
    "# # Generating model scores on validation images with best threshold\n",
    "# model_score(vgg_unet,processed_val_images,processed_val_labels,tprs,fprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hI2i7IDrx2Fu"
   },
   "outputs": [],
   "source": [
    "# # Predictions on the test set\n",
    "# test_predictions(vgg_unet,processed_test_images,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu0Lcq2sTlae"
   },
   "outputs": [],
   "source": [
    "# # Observing the results of 5-fold Cross_Validation with Normal Images\n",
    "# cv_vgg_unet = Vgg_Unet()\n",
    "# cv(m = vgg_unet, folds = 5, dataset = 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtdpAD5fJCqD",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Loading and training the VGG combined with UNET model with augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YI6cnrwyvkD",
    "outputId": "fe87071c-734d-43c6-8939-f06bbd26216f"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "aug_vgg_unet = Vgg_Unet()\n",
    "\n",
    "# Getting images\n",
    "imgs = img('augmented')\n",
    "\n",
    "# Train the model\n",
    "h_aug_vgg_unet = aug_vgg_unet.fit(imgs[0],imgs[1],epochs=epochs, batch_size=batch_size, validation_data = (imgs[2],imgs[3]))\n",
    "\n",
    "# Storing the history\n",
    "model_histories['aug_vgg_unet'] = h_aug_vgg_unet\n",
    "\n",
    "# Plotting the accuracy and loss for train and validation data\n",
    "# acc_loss_plot(h_aug_vgg_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "uf-622q6zMaI",
    "outputId": "0da8e0b7-4d6f-483a-e6f1-7e0b8d69a86e"
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "tprs,fprs = roc_curve(aug_vgg_unet,processed_val_images,processed_val_labels)\n",
    "\n",
    "# Get best threshold on base of tprs and fprs\n",
    "threshold = thr_calc(tprs,fprs)\n",
    "\n",
    "# Storing the model, tprs, fprs\n",
    "models['aug_vgg_unet'] = [aug_vgg_unet,tprs,fprs,threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12XFm5lQdxyH"
   },
   "outputs": [],
   "source": [
    "# # Generating model scores on validation images with best threshold\n",
    "# model_score(aug_vgg_unet,processed_val_images,processed_val_labels,tprs,fprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oc_Nbv7Zzxtj"
   },
   "outputs": [],
   "source": [
    "# # Predictions on the test set\n",
    "# test_predictions(aug_vgg_unet,processed_test_images,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gcP6hDCTzTM"
   },
   "outputs": [],
   "source": [
    "# # Observing the results of 5-fold Cross_Validation with Augmented Images\n",
    "# cv_vgg_unet = Vgg_Unet()\n",
    "# cv(m = cv_vgg_unet,folds = 5,dataset = 'augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYW6YDIf6tnC",
    "tags": []
   },
   "source": [
    "### Fine Tuned MobileNet, Combined with Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mobilenet was a lightweight experimentation alternative of VGG16, for quick and efficient training.\n",
    "- Turns out that it performs good with augmented images, as compared to basic UNET but a bit poor than VGG_UNET.\n",
    "    - This can be seen especially in predictions where Mobilenet puts sparkles in no-roof places, and is not able to map roof linings well\n",
    "    - Mobilenet does a good job at getting an irregular square shape of rooftops, which is better than UNET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhVOyl7FmVU3"
   },
   "outputs": [],
   "source": [
    "def mobunet_model(input_size=(224, 224, 3), n_filters=16, n_classes=1,dropout_prob = 0):\n",
    "  '''\n",
    "  Returns a MobUnet Architecture Model\n",
    "  @param: input_size: input size of image\n",
    "  @param: n_filters: Number of filters to be used in architecture layers\n",
    "  @param: n_classes: Total Classes to be predicted\n",
    "  @param: dropout_prob: Dropout probabilities\n",
    "  '''  \n",
    "  # Using a whole pretrained mobileNet V2 architecture\n",
    "  base_model = MobileNetV2(input_shape=[224, 224, 3], include_top=False)\n",
    "\n",
    "  # Specify the layers whose activations will be used as skip connections\n",
    "  layer_names = [\n",
    "        'block_1_expand_relu',   # 64x64\n",
    "        'block_3_expand_relu',   # 32x32\n",
    "        'block_6_expand_relu',   # 16x16\n",
    "        'block_13_expand_relu',  # 8x8\n",
    "        'block_16_project',      # 4x4\n",
    "    ]\n",
    "  \n",
    "  # Retrieve the output tensors of the specified layers\n",
    "  base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "  # Create the feature extraction model\n",
    "  down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "  down_stack.trainable = False\n",
    "\n",
    "  # Define the upsampling layers\n",
    "  up_stack = [\n",
    "        pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "        pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "        pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "        pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "    ]\n",
    "\n",
    "  # Define the input layer\n",
    "  inputs = Input(shape=input_size)\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = down_stack(inputs)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  # Append another Conv2DTranspose layer with appropriate parameters\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3,3), strides=2, padding='same')(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D( filters=n_classes, kernel_size=(1,1), strides=1, padding='valid')(x)\n",
    "    \n",
    "  \n",
    "  # Apply sigmoid activation function to obtain pixel-wise probabilities\n",
    "  x = tf.keras.activations.sigmoid(x)\n",
    "\n",
    "  x = Model(inputs = inputs, outputs = x)\n",
    "  x.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs7oo9zTJCqF",
    "tags": []
   },
   "source": [
    "#### Loading and training the MobileNet combined with UNET model without augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzu8S8VFJCqF",
    "outputId": "5593533c-a1ba-42d9-dcce-941aa9e8c0fb"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "mob_unet = mobunet_model()\n",
    "\n",
    "# Getting images\n",
    "imgs = img('normal')\n",
    "\n",
    "# Train the model\n",
    "h_mob_unet = mob_unet.fit(imgs[0],imgs[1],epochs=epochs, batch_size=batch_size, validation_data = (imgs[2],imgs[3]))\n",
    "\n",
    "# Storing the History\n",
    "model_histories['mob_unet'] = h_mob_unet\n",
    "\n",
    "# Plotting the accuracy and loss for train and validation data\n",
    "# acc_loss_plot(h_mob_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COUMXyu2KVgV"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(mob_unet, to_file='UNET_Mob.png', show_shapes=True, rankdir='TB')\n",
    "# layers = [(layer, layer.name, layer.trainable,layer.output) for layer in mob_unet.layers]\n",
    "# pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable', 'Layer Op'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "_6_yi9BX1CAt",
    "outputId": "c8815076-6b05-4175-85be-e2adcff6a3c7"
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "tprs,fprs = roc_curve(mob_unet,processed_val_images,processed_val_labels)\n",
    "\n",
    "# Get best threshold on base of tprs and fprs\n",
    "threshold = thr_calc(tprs,fprs)\n",
    "\n",
    "# Storing the model, tprs, fprs\n",
    "models['mob_unet'] = [mob_unet,tprs,fprs,threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMGyNKwC1UcA"
   },
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "# test_predictions(mob_unet,processed_test_images,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7ljupJIUOaW"
   },
   "outputs": [],
   "source": [
    "# # Observing the results of 5-fold Cross_Validation with Normal Images\n",
    "# cv_mob_unet = mobunet_model()\n",
    "# cv(m = cv_mob_unet,folds =  5, dataset = 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1iYE29BJCqG",
    "tags": []
   },
   "source": [
    "#### Loading and training the MobileNet combined with UNET model with augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asbyHfJO2YZn",
    "outputId": "2c3df434-bccd-4e86-c69f-366e1264ac12"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "aug_mob_unet = mobunet_model()\n",
    "\n",
    "# Getting images\n",
    "imgs = img('augmented')\n",
    "\n",
    "# Train the model\n",
    "h_aug_mob_unet = aug_mob_unet.fit(imgs[0],imgs[1],epochs=epochs, batch_size=batch_size, validation_data = (imgs[2],imgs[3]))\n",
    "\n",
    "# Storing the History\n",
    "model_histories['aug_mob_unet'] = h_aug_mob_unet\n",
    "\n",
    "# Plotting the accuracy and loss for train and validation data\n",
    "# acc_loss_plot(h_aug_mob_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "wfKCmOAXBRx9",
    "outputId": "13928d88-f207-4e3d-d4bb-6051622e7b88"
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curve \n",
    "tprs,fprs = roc_curve(aug_mob_unet,processed_val_images,processed_val_labels)\n",
    "\n",
    "# Get best threshold on base of tprs and fprs\n",
    "threshold = thr_calc(tprs,fprs)\n",
    "\n",
    "# Storing the model, tprs, fprs\n",
    "models['aug_mob_unet'] = [aug_mob_unet,tprs,fprs,threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhWXGhN-fERU"
   },
   "outputs": [],
   "source": [
    "# Generating model scores on validation images with best threshold\n",
    "# model_score(aug_mob_unet,processed_val_images,processed_val_labels,tprs,fprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHZAiEG92uQt"
   },
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "# test_predictions(aug_mob_unet,processed_test_images,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmDwidxGUdvy"
   },
   "outputs": [],
   "source": [
    "# # Observing the results of 5-fold Cross_Validation with Augmented Images\n",
    "# cv_mob_unet = mobunet_model()\n",
    "# cv(m = cv_mob_unet, folds = 5,datset = 'augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVvwTu0VloSU",
    "tags": []
   },
   "source": [
    "## Observations Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX6xfZKrdLT5"
   },
   "source": [
    "- [Link To Model Training Section](#Model-Training-Section)\n",
    "\n",
    "- This section is used to compare models on base of their training, validation accuracy and loss plots, ROC Curve and evaluate them on metrics of precision, recall and f1 score\n",
    "    - Observations are stated in a bit more detail regarding metrics in the section itself\n",
    "    - With observations 3 best models are chosen with their corresponding threshold\n",
    "\n",
    "- [Link To Best Models Section](#Best-Models-Section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VCedahWGw8j"
   },
   "source": [
    "#### Observations from above train and validation runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zXQ97ixJkBx"
   },
   "source": [
    "- By  looking at the val_accuracy, val_loss from model training we can infer:\n",
    "    - All models accuracy stay in range of 85(seen mostly on normal images) to 87(seen mostly on augmented images) on average\n",
    "    - For every model except CNN, the val loss increases quite a bit when augmented images are provided to model. This is more visible in transfer learning architectures.\n",
    "- Train Accuracy increases and Train losses decreases for every model over training and this trend increases more given the augmented images.\n",
    "\n",
    "- **Observation 1**: \n",
    "  - Given these trends, it becomes really hard to compare between the models on base of accuracy or loss. Also as we have converted our problem into binary classification, in an image, **the class ration of roof-no-roof is imbalanced**. Hence I decided to use metrics of precision, recall and f1-score, which are calculated by making pixel wise prediction on validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-MRVAesH5Mr"
   },
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gTTqhn0VloSU",
    "outputId": "3e82338f-5100-4768-c249-c4ee1cb9630d"
   },
   "outputs": [],
   "source": [
    "joint_plots(model_histories,epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "id": "w_0hskSRiozL",
    "outputId": "e56a0c23-dc2a-42d4-9df4-b12aa363a1d9"
   },
   "outputs": [],
   "source": [
    "joint_roc(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m9Zjbf4rzZdI",
    "outputId": "43d07d74-123e-4205-97f5-d36c7777b52c"
   },
   "outputs": [],
   "source": [
    "joint_preds(models,processed_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2DXScjVzmsA",
    "outputId": "1917a4a3-d2ca-43df-dcf4-b73fd65b6fd2"
   },
   "outputs": [],
   "source": [
    "joint_metrics(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1AwWJgbHheP"
   },
   "source": [
    "#### Observations from the metric values and prediction plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdDPDg3RHv7Z"
   },
   "source": [
    "- **Observation 2**: \n",
    "  - Predictions are always better for models trained on augmented images so we directly compare them\n",
    "  - aug_unet does a good job to draw circles on roofs\n",
    "  - aug_vgg_unet does even a better job in estimating corners as well with roofs\n",
    "  - aug_mob_unet does a good job, but has blurry borders on roofs, and also a bit of mis-classification on small patches in images.\n",
    "\n",
    "- **Observation 3**:\n",
    "  - By being inline with observation 1 and with this understanding \n",
    "    - For rooftop classification, let's consider an onsite engineer, is going to visit the areas where we have more rooftops, so if our model is producing more FN's we are missing more rooftop areas, while if FP's are more, we just miss a trip of engineer, not an actual client. So I am assuming FN should be less, i.e. Recall has to be higher.\n",
    "    - Also to balance this since I am not completely knowing usecase I will F-1 Score as second parameter of choice.\n",
    "  - Models with good F-1 Score: aug_vgg_unet, aug_unet, aug_mob_unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd1uvnZ7dLT6"
   },
   "source": [
    "## Best Models Section\n",
    "- [Link To Observations Section](#Observations-Section)\n",
    "\n",
    "- This section simply includes training of best models on whole training data both normal and augmented. After training visual prediction plots are generated with optimal threshold values obtained during model predictions on validation sets.\n",
    "\n",
    "- [Link To References and Conculsion Section](#References-and-Conclusions-Section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ts2xHK8ZdLT6"
   },
   "source": [
    "### By above observations, we have 3 best models aug_vgg_unet, aug_unet, aug_mob_unet, lets train them on whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAM9x-tZI6XH",
    "outputId": "24d237f5-7817-45dc-9587-ca3630295676"
   },
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/images/dida_task_dummy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9rKPPhyLuPY",
    "outputId": "9f8ba1c1-6113-419b-95f9-5d4068726286"
   },
   "outputs": [],
   "source": [
    "# Directories of images\n",
    "train_image_path = './train/images'\n",
    "train_label_path = './train/labels'\n",
    "test_image_path = './test/images'\n",
    "\n",
    "# Loading the images from dirs\n",
    "# We are choosing this size of 224 so that Maxpooling works well\n",
    "train_images = train_test_img_loader(train_image_path,224) \n",
    "train_labels = label_img_loader(train_label_path,224)\n",
    "test_images = train_test_img_loader(test_image_path,224)\n",
    "\n",
    "# Normalizing the images and making label images having enough channels for NN.\n",
    "processed_train_images = train_images.astype('float32')\n",
    "processed_test_images = test_images.astype('float32')\n",
    "processed_train_labels = train_labels\n",
    "\n",
    "# Augmenting the images, generating 6 augmentations per image\n",
    "# Chosen 6 as lower than that decreases performance and higher than that overfits the model\n",
    "train_aug_images, train_aug_labels = augment_images(processed_train_images,processed_train_labels,num_aug = 6)\n",
    "print(train_aug_images.shape, train_aug_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmxWFsVbYet3"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 5\n",
    "bst_models = {}\n",
    "# bs: stands for base\n",
    "# bst: stands for base transformed, i.e model uses transformed images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training best models on normal and augmented images images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "bs_unet = Unet()\n",
    "\n",
    "# Train the model\n",
    "hbs_unet = bs_unet.fit(processed_train_images,processed_train_labels, epochs = epochs, batch_size = batch_size)\n",
    "thr = models['unet'][3] #Getting the thereshold value from the trained models dictionary\n",
    "\n",
    "# Storing the models information\n",
    "model_histories['bs_unet'] = hbs_unet\n",
    "bst_models['bs_unet'] = [bs_unet,thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaB2DorIY-L7",
    "outputId": "5a60eda9-cda7-4377-b3d4-7055e3b5be5f"
   },
   "outputs": [],
   "source": [
    "# Define a model\n",
    "bst_unet = Unet()\n",
    "\n",
    "# Train the model\n",
    "hbst_unet = bst_unet.fit(train_aug_images,train_aug_labels, epochs = epochs, batch_size = batch_size)\n",
    "thr = models['aug_unet'][3] #Getting the thereshold value from the trained models dictionary\n",
    "\n",
    "# Storing the models information\n",
    "model_histories['bst_unet'] = hbst_unet\n",
    "bst_models['bst_unet'] = [bst_unet,thr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### VGG_Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "bs_vgg_unet = Vgg_Unet()\n",
    "\n",
    "# Train the model\n",
    "hbs_vgg_unet = bs_vgg_unet.fit(processed_train_images,processed_train_labels, epochs = epochs, batch_size = batch_size)\n",
    "thr = models['vgg_unet'][3] #Getting the thereshold value from the trained models dictionary\n",
    "\n",
    "# Storing the models information\n",
    "model_histories['bs_vgg_unet'] = hbs_vgg_unet\n",
    "bst_models['bs_vgg_unet'] = [bs_vgg_unet,thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrBcxc0oY_PP",
    "outputId": "ab449d61-a388-4676-f23d-c7a1b25648ba"
   },
   "outputs": [],
   "source": [
    "# Define a model\n",
    "bst_vgg_unet = Vgg_Unet()\n",
    "thr = models['aug_vgg_unet'][3]\n",
    "\n",
    "# Train the model\n",
    "hbst_vgg_unet = bst_vgg_unet.fit(train_aug_images,train_aug_labels, epochs = epochs, batch_size = batch_size)\n",
    "\n",
    "# Storing the models information\n",
    "model_histories['bst_vgg_unet'] = hbst_vgg_unet\n",
    "bst_models['bst_vgg_unet'] = [bst_vgg_unet,thr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mob_Unet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "bs_mob_unet = mobunet_model()\n",
    "thr = models['mob_unet'][3]\n",
    "\n",
    "# Train the model\n",
    "hbs_mob_unet = bs_mob_unet.fit(processed_train_images,processed_train_labels, epochs = epochs, batch_size = batch_size)\n",
    "\n",
    "# Storing the models information\n",
    "model_histories['bs_mob_unet'] = hbs_mob_unet\n",
    "bst_models['bs_mob_unet'] = [bs_mob_unet,thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgVPvo6iY_7d",
    "outputId": "b85d07b8-99ee-4e0f-bb86-d5b8660e1669"
   },
   "outputs": [],
   "source": [
    "# Define a model\n",
    "bst_mob_unet = mobunet_model()\n",
    "thr = models['aug_mob_unet'][3]\n",
    "\n",
    "# Train the model\n",
    "hbst_mob_unet = bst_mob_unet.fit(train_aug_images,train_aug_labels, epochs = epochs, batch_size = batch_size)\n",
    "\n",
    "# Storing the models information\n",
    "model_histories['bst_mob_unet'] = hbst_mob_unet\n",
    "bst_models['bst_mob_unet'] = [bst_mob_unet,thr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgI__ZQlZPuU"
   },
   "source": [
    "### Predictions on the test data with these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bzONLtQ_Zjj4",
    "outputId": "bf738141-695e-4ebe-d953-c8be4be6f968"
   },
   "outputs": [],
   "source": [
    "bst_joint_preds(bst_models,processed_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PdShz8IdLT6"
   },
   "source": [
    "## References and Conclusions Section\n",
    "- [Link To Best Models Section](#Best-Models-Section)\n",
    "\n",
    "- This section includes references I went through during this segmentation tasks, and conclusions I made from this whole task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewhwNAu_S6LS"
   },
   "source": [
    "- After the training with all the given images, a slight imporvement can be spotted in terms of finding a roof, like the roof parts which were not covered fully previously are now covered more efficiently.\n",
    "- Also we can see the models overfitted a bit to this training data, cause we can see red spots to the points where ther are no roofs, like pathches on the streets and green fields a bit.\n",
    "- However I believe given small amount of data, this is the best I was able to make of problem statement and NN's, hope to hear from you soon Dida team with a feedback or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEjx5_fudLT7"
   },
   "source": [
    "- [Understanding Semantic Segmentation with UNET](https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47)\n",
    "- [Transfer Learning and Unet to segment rocks on moon](https://www.kaggle.com/code/basu369victor/transferlearning-and-unet-to-segment-rocks-on-moon)\n",
    "- [U-Net Implementation from Scratch using TensorFlow](https://github.com/VidushiBhatia/U-Net-Implementation/tree/main)\n",
    "- [Metrics for Classification](https://developers.google.com/machine-learning/crash-course/classification/)\n",
    "- Chat GPT if I felt stuck on understanding some concepts"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Fd1amPZscNbB",
    "ZjloPmupPd7Y",
    "Ko9iIW6GJCp5",
    "OR0xMSJKP4DC",
    "n64jFX78JCp6"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
